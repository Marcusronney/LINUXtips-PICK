# LINUXtips-PICK
Projeto final do Programa Intensivo em Containers e Kubernetes | PICK LINUXtips 


## INFRAESTRUTURA

No meu projeto, o Cluster irÃ¡ rodar localmente em uma VM no Hyper-V.

# EspecificaÃ§Ãµes:

|       VM       |       CPU        |     RAM     |     OS      |
|----------------|------------------|-------------|-------------|
|     Hyper-V    | Xeon 2360 4 Core |     17GB    | Rocky Linux |  


## PROVISIONANDO SERVER PARA O CLUSTER.

# Rede:

Irei configurar o IP 192.168.1.81/24 de forma static na subrede 192.168.1.x junto com meu gateway, o DNS serÃ¡ do Google (8.8.8.8).

```
nmcli con mod eth0 ipv4.addresses 192.168.1.81/24
nmcli con mod eth0 ipv4.gateway 192.168.1.254
nmcli con mod eth0 ipv4.dns 8.8.8.8
nmcli con mod eth0 ipv4.method manual
```

Reinicie a conexÃ£o para aplicar as alteraÃ§Ãµes.

*nmcli con down eth0 && nmcli con up eth0*

Visualizar:

*ip a show eth0
nmcli dev show eth0*


Rede Configurada. -----------------------------------------------------------------


Vamos atualizar o OS.

```
dnf update -y
```

Instale o Git:
```
sudo dnf install -y git
```


# VM atualizada e Git instalado, agora vamos para o Docker.

# Docker

```
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
```
# Vamos instalar o Conterinerd
```
sudo dnf install -y docker-ce docker-ce-cli containerd.io
```

Habilite o service do docker na inicializaÃ§Ã£o.
```
sudo systemctl enable docker --now
```
Verifique a instalaÃ§Ã£o

*docker --version*

![Title](imagens/docker/docker_instalado.png)



# Instalando Kubectl

Vamos instalar a versÃ£o stable do kubectl, dar permissÃ£o para execuÃ§Ã£o e mover a saÃ­da para o diretÃ³rio */usr/local/bin/*
```
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
```

Verifique a instalaÃ§Ã£o

*kubectl version --client*

![Title](imagens/kubectl/kubectl.png)



# Deploy KinD.

Vamos baixar o kind, dar permissÃ£o de execuÃ§Ã£o e mover para o diretÃ³rio */usr/local/bin/kind*
```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

Verifique a instalaÃ§Ã£o:

*kind --version*

![Title](imagens/kind/kind.png)


Kind instalado com sucesso. Vamos criar um manifest para deploy do Cluster.


kind-config.yaml
```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  apiServerAddress: "192.168.1.81"  # IP externo do host
  apiServerPort: 17443
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"

nodes:
  - role: control-plane
    extraPortMappings:
      - containerPort: 80
        hostPort: 80
      - containerPort: 443
        hostPort: 443
      - containerPort: 6443
        hostPort: 17445  # Porta do servidor API

  - role: worker
```



# Deploy:
```
kind create cluster --name giropops-cluster --config cluster-config.yaml
```

![Title](imagens/kind/1.png)



Vamos verificar se o Cluster subiu corretamente.

Cluster:
*kubectl cluster-info --context kind-giropops-cluster*

Nodes:
*kubectl get nodes*

Se tudo estiver OK, vocÃª verÃ¡:

    O endereÃ§o do API server https://192.168.1.81:17443

    Dois nodes: control-plane e worker com STATUS: Ready


![Title](imagens/kind/2.png)

Cluster funcionando.


Vamos preparar a nossa imagem para deploy.






# DOCKER

Iremos criar um Dockerfile Single-Stage Runtime para a aplicaÃ§Ã£o.


Vamos criar um **Dockerfile** dentro do diretÃ³rio da aplicaÃ§Ã£o.

***Estrutura para o Dockerfile:***

giropops-senhas/

â”œâ”€â”€ app.py

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ templates/

â”œâ”€â”€ static/

â””â”€â”€ Dockerfile




Dockerfile

```
FROM python:3.12-slim #estamos definindo a imagem oficinal Python

WORKDIR /app #Aqui definimos o diretÃ³rio principal do container, todos os comandos agora irÃ£o rodar a parti do /app

COPY requirements.txt . #/app/requirements.txt
COPY app.py . #/app/app.py
COPY templates templates/ #/app/templates/
COPY static static/ #/app/static/

RUN pip install --no-cache-dir -r requirements.txt #aqui, vamos instalar as dependÃªncias dentro do .txt

EXPOSE 5000 #o container irÃ¡ escutar na porta 5000

CMD ["flask", "run", "--host=0.0.0.0"] #definimos um comando para subir o flask e aceitar conexÃ£o de qualquer ip.

```


Vamos Buildar a aplicaÃ§Ã£o.

```
docker build -t giropops-senhas .
```

Agora vamos expor o container na porta 5000 e testar a aplicaÃ§Ã£o.
```
docker run -p 5000:5000 giropops-senhas
```

![Title](imagens/docker/buildgiro.png)


AplicaÃ§Ã£o funcionando.

![Title](imagens/docker/pops.png)

Agora que criamos e buildamos a aplicaÃ§Ã£o giropops-senhas no Docker, vamos analisar a imagem.

Usando o comando *docker images* e *docker history* conseguimos visualizar a imagem criada e o seu processo de build. Note que a imagem possuÃ­ um tamanho de 140MB.

![Title](imagens/docker/dockerimage.png)

SaÃ­da do docker history:
```
IMAGE          CREATED         CREATED BY                                 SIZE      COMMENT
527daaa695dc   11 minutes ago  CMD ["flask" "run" "--host=0.0.0.0"]        0B
<missing>      11 minutes ago  EXPOSE map[5000/tcp:{}]                    0B
<missing>      11 minutes ago  RUN pip install --no-cache-dir ...         15.3MB
<missing>      11 minutes ago  COPY static static/                        101kB
<missing>      11 minutes ago  COPY templates templates/                  5.78kB
<missing>      11 minutes ago  COPY app.py .                              5.59kB
<missing>      1 hour ago      COPY requirements.txt .                    51B
<missing>      1 hour ago      WORKDIR /app                               0B
<missing>      7 days ago      FROM python:3.12-slim                      74.8MB (base image + dependÃªncias)
```
![Title](imagens/docker/dockerhistory.png)

Aqui podemos visualizar todas as camadas do processo de build da nossa aplicaÃ§Ã£o. Uma imagem Single-Stage, tamanho um pouco elevado e com uma superfÃ­cie de ataque elevada.



# Agora, iremos fazer o build da aplicaÃ§Ã£o utilizando Melange + APKO.



### MELANGE

O Melange Ã© uma ferramenta para construir pacotes para sistemas baseados em 
Alpine Linux e APKO. Ele permite que vocÃª crie pacotes .apk que podem ser incluÃ­dos em imagens APKO e 
usados em containers leves.


Vamos instalar o Melange.

curl -L https://github.com/chainguard-dev/melange/releases/download/v0.23.6/melange_0.23.6_linux_amd64.tar.gz -o melange.tar.gz
tar -xzf melange.tar.gz

Arquivo extraÃ­do, vamos entrar no diretÃ³rio, dar permissÃ£o e mover para /ur/local/bin/melange

cd melange_0.23.6_linux_amd64/

#ls
LICENSE  melange

mv melange /usr/local/bin/
chmod +x /usr/local/bin/melange

![Title](imagens/melange/melange.png)


Teste o Melange:

*melange version*


![Title](imagens/melange/1.png)


#APKO

```
curl -L https://github.com/chainguard-dev/apko/releases/download/v0.10.0/apko_0.10.0_linux_amd64.tar.gz -o apko.tar.gz
tar -xzf apko.tar.gz
cd apko_0.10.0_linux_amd64/
chmod +x apko
sudo mv apko /usr/local/bin/
```

![Title](imagens/melange/apko.png)


Verifique a versÃ£o:

*apko version*


![Title](imagens/melange/apko1.png)



**MELANGE e APKO Instalado com sucesso!**



Gere as chaves:
```
melange keygen
```

![Title](imagens/melange/3.png)

Agora possuÃ­mos 2 chaves, uma privada "melange.rsa" e outra pÃºblica "melange.rsa.pub".


RenomÃ©ie a chave pÃºblica:
```
mkdir keys
cp melange.rsa.pub melange.key
```





Vamos criar nosso manifesto melange.yaml

vi melange.yaml

```
package:
  name: giropops-senhas
  version: 0.1
  description: AplicaÃ§Ã£o Giropops-senhas - Gerador de senhas - LinuxTips
  dependencies:
    runtime:
      - python3

environment:
  contents:
    keyring:
      - ./melange.rsa.pub
    repositories:
      - https://dl-cdn.alpinelinux.org/alpine/edge/main
      - https://dl-cdn.alpinelinux.org/alpine/edge/community
    packages:
      - alpine-baselayout-data
      - ca-certificates-bundle
      - busybox
      - gcc
      - musl-dev
      - python3
      - python3-dev
      - py3-pip
      - py3-virtualenv
pipeline:
  - name: Build Python application
    runs: |
      EXECDIR="${{targets.destdir}}/usr/bin"
      WEBAPPDIR="${{targets.destdir}}/usr/share/webapps/giropops-senhas"
      mkdir -p "${EXECDIR}" "${WEBAPPDIR}"
      echo "#!/usr/share/webapps/giropops-senhas/venv/bin/python3" > "${EXECDIR}/giropops-senhas"
      cat app.py >> "${EXECDIR}/giropops-senhas"
      chmod +x "${EXECDIR}/giropops-senhas"
      virtualenv "${WEBAPPDIR}/venv"
      cp -r templates/ static/ ${WEBAPPDIR}/
      sh -c "source '${WEBAPPDIR}/venv/bin/activate' && pip install -r requirements.txt"
```


Resumo do que ele faz:

    Define um pacote chamado giropops-senhas, versÃ£o 0.1.

    Declara dependÃªncias do sistema necessÃ¡rias para compilar e rodar uma app Python (gcc, musl, python3, etc).

    Cria uma pipeline de build, que:

        Prepara diretÃ³rios destino.

        Copia e monta o script app.py como um executÃ¡vel (giropops-senhas).

        Cria um ambiente virtual Python (venv) no diretÃ³rio de webapp.

        Copia pastas templates/ e static/.

        Instala as dependÃªncias do requirements.txt.

    
Agora, vamos criar o manifesto do apko.

vi apko.yaml
```
contents:
  repositories:
    - https://dl-cdn.alpinelinux.org/alpine/edge/main
    - /work/packages
  packages:
    - alpine-baselayout
    - giropops-senhas
    - curl

accounts:
  groups:
    - groupname: nonroot
      gid: 65532
  users:
    - username: nonroot
      uid: 65532
      gid: 65532
  run-as: 65532

environment: 
  FLASK_APP: "/usr/bin/giropops-senhas"

entrypoint:
  command: /usr/bin/giropops-senhas

archs:
  - x86_64
```

Crie o diretÃ³rio mkdir packages/

*mkdir packages*

Estrutura:
giropops-senhas/
â”œâ”€â”€ app/                        # CÃ³digo-fonte da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ static/
â”œâ”€â”€ melange.yaml         # Receita do pacote .apk
â”œâ”€â”€ apko.yaml            # Receita da imagem
â”œâ”€â”€ melange.key          # Chave pÃºblica
â”œâ”€â”€ melange.rsa          # Chave privada
|   melange.rsa.pub      # Chave pÃºblica
â””â”€â”€ packages/            # Onde o .apk serÃ¡ salvo
â”œâ”€â”€ output/              # Gerado automaticamente com os pacotes
    â””â”€â”€ packages/x86_64/



Vamos Buildar a imagem.



Empacotando imagem com Melange via Docker.
```
docker run --rm --privileged \
  -v "${PWD}:/work" \
  -w /work \
  cgr.dev/chainguard/melange \
  build melange.yaml \
  --arch x86_64 \
  --signing-key ./keys/melange.rsa \
  --repository-append ./packages
```

Os campos *INFO wrote packages/x86_64/giropops-senhas-0.1-r0.apk* e 
*INFO writing signed index to packages/x86_64/APKINDEX.tar.gz* indicam que o empacotamento foi realizado com sucesso.

![Title](imagens/melange/buildmelange.png)


Agora possuÃ­mos o .apk da aplicaÃ§Ã£o.

ÃŒndice assinado e o diretÃ³rio *packages/x86_64/* pronto para o apko.
```
#ls packages/x86_64/

APKINDEX.tar.gz  giropops-senhas-0.1-r0.apk
```



Realizando Build com APKO:

```
docker run --rm \
  -v "${PWD}:/work" \
  -w /work \
  cgr.dev/chainguard/apko \
  build apko.yaml giropops-senhas giropops.tar \
  -k melange.rsa.pub
```

apko.yaml â†’ especifica o que vai na imagem (pacotes, entrypoint etc.)

giropops-senhas â†’ nome lÃ³gico da imagem

giropops.tar â†’ imagem gerada como tarball

-k melange.rsa.pub â†’ chave pÃºblica usada para verificaÃ§Ã£o de pacotes

**Imagem APKO ConstruÃ­da com sucesso.**

![Title](imagens/melange/5.png)


Verifique o arquivo gerado: 
```
ls -lh giropops.tar
```
![Title](imagens/melange/4.png)



# SUBINDO IMAGEM APKO para o DOCKER HUB.

Vamos carregar a imagem.tar para nosso repositÃ³rio local:

*docker load < giropops.tar*

![Title](imagens/melange/apko_docker.png)

Realizando um teste da imagem empagotada via APKO:

```
docker run -p 5000:5000 giropops-senhas:latest-amd64
```

# Build Melange e empacotamento APKO realizado com sucesso!
![Title](imagens/melange/buildapko.png)



# Docker HUB

Vamos realizar login no Docker Hub, definir uma tag para a imagem criada e fazer push.
```
docker login
docker tag
docker push
```




Imagem APKO upada no Docker Hub com apenas 25.75 MB.

![Title](imagens/melange/dockerhub.png)




# TRIVY - AnÃ¡lise de Vulnerabilidades



Instalando:
```
curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin

aquasecurity/trivy info checking GitHub for latest tag
aquasecurity/trivy info found version: 0.61.0 for v0.61.0/Linux/64bit
aquasecurity/trivy info installed /usr/local/bin/trivy
```
![Title](imagens/trivy/trivy.png)


Vamos verificar as vulnerabilidades da nossa imagem AKPO.
```
trivy image --severity HIGH,CRITICAL --ignore-unfixed geforce8400gsd/giropops-senhas
```

![Title](imagens/trivy/scan1.png)


**0 Vulnerabilidades**.

Agora, vamos comparar as vulnerabilidades da nossa imagem APKO com o nosso Dockerfile.


![Title](imagens/trivy/trivy_giropops.png)

**105 Vulnerabilidades**.


### Comparativo de Imagens - AKPO vs Docker

Agora possuÃ­mos uma imagem Ultra-minimalista com apenas 20 MB e com uma superfÃ­cie de ataques reduzida, possuindo apenas o necessÃ¡rio. Note que diferenÃ§a de vulnerablidades, de 105 para 0.

| CritÃ©rio                   | Dockerfile ClÃ¡ssico (`python:3.12-slim`) | Melange + APKO (`alpine`)           |
|---------------------------|-------------------------------------------|-------------------------------------|
| ðŸ“¦ Imagem base            | python:3.12-slim (~74MB)                  | alpine (~5MB base)                  |
| ðŸ“ Tamanho final da imagem| ~140MB                                    | ~20-25MB                            |
| ðŸ›¡ï¸ SeguranÃ§a               | UsuÃ¡rio root                              | UsuÃ¡rio **nÃ£o-root** (UID 65532)    |
| ðŸ§¼ Imagem limpa           | ContÃ©m pip, gcc, cache, etc.              | SÃ³ o necessÃ¡rio, nada de build tools|
| ðŸ” Reprodutibilidade      | Parcial                                   | **Total (com assinatura RSA)**      |
| ðŸ” Supply Chain Security  | NÃ£o possui verificaÃ§Ã£o de pacotes         | **Melange + assinatura de pacotes** |
| ðŸ”§ Complexidade           | Baixa (fÃ¡cil de aprender)                 | Moderada (curva de aprendizado maior)|
| Vulnerabilidades           | 105                                       | 0                                    |

------------------------------------------------------------------


# Kubernetes

Buildando a imagem no Kubernetes com KinD.

KinD = â€œKubernetes IN Dockerâ€
Ã‰ uma forma super leve de rodar um cluster Kubernetes completo dentro de containers Docker.


Como nosso KiND jÃ¡ estÃ¡ instalado, vamos parti para o build da aplicaÃ§Ã£o. 

Verificando o Cluster.

O KinD Ã© um Kubernetes no qual executa o cluster via containers com Docker.

Verifique os containers Kind:
```
docker ps -a
```
![Title](imagens/kind/dockerps-a.png)

Container *giropops-cluster-worker* e *giropops-cluster-control-plane* rodando perfeitamente.




Verifique o Cluster:
```
kind get clusters
```
![Title](imagens/kind/dockerps-a.png)


Verifique as informaÃ§Ãµes do Cluster e os Nodes
```
kubectl cluster-info
kubectl get nodes
```

![Title](imagens/kind/nodes_info.png)

Por Ãºltimo, verifique os Pods do Kubernetes:

```
kubectl get pods -A
```

![Title](imagens/kind/kubectlgetpods-A.png)

Agora podemos visualizar a menor unidade do nosso Cluster Kubernetes, os PODs, em resumo:

coredns (2 pods)	DNS interno do cluster

etcd	Banco de dados do cluster

kube-apiserver	API principal do Kubernetes

kube-controller-manager	Controlador de recursos

kube-scheduler	Agenda pods nos nodes

kube-proxy (2 pods)	Regras de rede por node

kindnet (2 pods)	CNI de rede do KinD

Conseguimos visualizar a saÃºde do nosso Cluster, todos os Pods necessÃ¡rios para o cluster funcionar estÃ£o rodando perfeitamente na namespace kube-system.



----------------------------

Buildando a imagem Kubernetes.

Vamos realizar o build e teste da aplicaÃ§Ã£o.

Manifestos:

giropops-deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: giropops-senhas
  name: giropops-senhas
spec:
  replicas: 1
  selector:
    matchLabels:
      app: giropops-senhas
  template:
    metadata:
      labels:
        app: giropops-senhas
    spec:
      containers:
      - image: geforce8400gsd/giropops-senhas:latest
        name: giropops-senhas
        env:
        - name: REDIS_HOST
          value: redis-service
        ports:
        - containerPort: 5000
```

giropops-service.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: giropops-senhas-service
spec:
  type: NodePort
  selector:
    app: giropops-senhas
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
    nodePort: 32000  # Escolha uma porta diferente
```

redis-deployment.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379
  type: ClusterIP
```

redis-service.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: redis
  name: redis-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - image: redis
        name: redis
        ports:
          - containerPort: 6379
        resources:
          limits:
            memory: "256Mi"
            cpu: "500m"
          requests:
            memory: "128Mi"
            cpu: "250m"
```

Build:
```
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl apply -f redis-deployment.yaml
kubectl apply -f redis.yaml
```

Manifestos yaml buildados, vamos visualizar os Pods e Services.

```
kubectl get pods --all-namespaces
```
```
kubectl get svc --all-namespaces
```
Agora possuÃ­mos os Pods e Services da aplicaÃ§Ã£o e do Redis, vamos expor via *port-foward* e testar o acesso na aplicaÃ§Ã£o.
![Title](imagens/kubernetes/kubectl.png)


Port-Foward:
```
kubectl port-forward deployment/giropops-senhas 5000:5000
```
![Title](imagens/kubernetes/port-foward.png)


**AplicaÃ§Ã£o Buildada e acessada no Kubernetes com sucesso.**
![Title](imagens/kubernetes/aplicacao_kubernetes.png)






# HELM - GestÃ£o de Pacotes do Kubernetes

O Helm Ã© uma ferramenta open-source que permite gerenciar aplicaÃ§Ãµes Kubernetes de forma simples e eficiente. Com o Helm, vocÃª pode instalar, atualizar e desinstalar aplicaÃ§Ãµes em um cluster Kubernetes com facilidade.



**Instalando HELM:**
```
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```
Verifique a versÃ£o:
```
HELM version
```
![Title](imagens/helm/helminstall.png)


------

**Estruturando Cluster com HELM**

Definindo a estrutura do Projeto.
```
giropops-senhas/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml (base)
â”œâ”€â”€ values-dev.yaml
â”œâ”€â”€ values-prod.yaml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ deploymentlocust.yaml
â”‚   â”œâ”€â”€ redis-deployment.yaml
â”‚   â”œâ”€â”€ redis-service.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ service-locust.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ locust-configmap.yaml
â”‚   â””â”€â”€ _helpers.tpl
```

No projeto, um dos requisitos Ã© ter 3 ambientes, um de produlÃ§ao, outro de teste e outro de desenvolvimento. A separaÃ§Ã£o dos ambientes serÃ¡ feita atravÃ©s dos manifestos values, exemplo: values-prod.yaml, values-dev.yaml e values-staging.yaml.

Criando as Namespaces:
```
kubectl create namespace dev
kubectl create namespace staging
kubectl create namespace prod
```
![Title](imagens/kubernetes/ns.png)


**Deploy Helm:** values-dev.yaml, values-staging.yaml e values-prod.yaml

Desenvolvimento:
```
helm upgrade --install giropops-dev . \
  --namespace dev \
  --values values-dev.yaml
```
Teste:
```
helm install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml 
```
ProduÃ§Ã£o:
```
helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```

![Title](imagens/helm/deploy.png)


Agora possuÃ­mos as Namespaces nos 3 ambientes, prod, dev e staging.
```
helm list
```
![Title](imagens/helm/helmlist.png)


Verificando os pods:
```
kubectl get pods -n dev
kubectl get pods -n staging
kubectl get pods -n prod
```
![Title](imagens/helm/helmlistpod.png)


Vamos verificar os pods criados:
```
kubectl get all --all-namespaces
```
![Title](imagens/helm/all.png)


Um detalhe, no values-dev.yaml definimos **hpa:
  enabled: false** para que o HPA funcione apenas nos ambientes prod e staing.
![Title](imagens/helm/hpa.png)











# NGINX INGRESS - Expondo o Cluster


Ingress Ã© um recurso do Kubernetes que gerencia o acesso externo de um serviÃ§o dentro do Cluster.

Ele funciona como uma camada de Roteamento HTTP/HTTPS, permitindo a definiÃ§Ã£o de regras para direcionar
o trÃ¡fego externo para diferentes serviÃ§os back-end.

Como ele irÃ¡ funcionar?
```
Navegador â†“
http://giropops.local:32080
     â†“
Ingress Controller (NGINX)
     â†“
Ingress Rule (roteamento)
     â†“
Service (ex: giropops-senhas)
     â†“
Pod (sua aplicaÃ§Ã£o Flask ou FastAPI ou NodeJS etc)

```

InstalaÃ§Ã£o Nginx Ingress:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/kind/deploy.yaml
```


Verifique os Pods Nginx.

O NGINX Controller, por padrÃ£o, nÃ£o especifica um nodeSelector, mas Ã s vezes os taints/tolerations ou a falta de afinidade impedem ele de ser escalonado nesse Ãºnico node.
Se o pod nÃ£o iniciar, rode:
```
kubectl patch deployment ingress-nginx-controller -n ingress-nginx \
  --type='json' -p='[{
    "op": "add",
    "path": "/spec/template/spec/nodeSelector",
    "value": {
      "kubernetes.io/hostname": "giropops-cluster-control-plane"
    }
  }]'
```


```
kubectl get pods -n ingress-nginx
```
![Title](imagens/ingress/ingressnginx.png)

Vamos realizar o Deploy do Ingress NGINX com as portas 32080 (HTTP) e 32443 (HTTPS)


Agora vamos realizar deploy do manifestos ingress.yaml.

ingress.yaml
```
{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: giropops-ingress
  namespace: {{ .Release.Namespace }}
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    {{- if .Values.ingress.allowIpAccess }}
    nginx.ingress.kubernetes.io/whitelist-source-range: "192.168.1.0/24"
    {{- end }}
spec:
  ingressClassName: nginx
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ .Values.ingress.serviceName }}
                port:
                  number: {{ .Values.ingress.servicePort }}
{{- end }}
```

Para expor todos os Cluster com 1 manifesto Ingress foi definido as especificaÃ§Ãµes via vÃ¡riaveis.

No Manifesto ingress defini vÃ¡riaveis irÃ¡ buscar o valor em cada 1 dos values.
Exemplo Ingress.yaml:
```
name: {{ include "giropops.fullname" . }}-ingress
service:
                name: {{ .Values.ingress.serviceName }}
                port:
                  number: {{ .Values.ingress.servicePort }}
```
values-dev.yaml:
```
ingress:
  enabled: true
  host: dev.giropops.local
  serviceName: giropops-senhas-giropops-senhas-port
  servicePort: 5000
  allowIpAccess: true
```




Agora, irei fazer upgrade do HELM para atualizar os 3 Ambientes prod, staging e dev:
```
helm upgrade --install giropops-dev . \
  --namespace dev \
  --values values-dev.yaml

helm upgrade --install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml

helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```

Agora irei verificar os ingress criados:
```
kubectl get ingress -n dev
kubectl get ingress -n prod
kubectl get ingress -n staging
```
![Title](imagens/ingress/ingress-n.png)


Vamos verificar os Pods Nginx Ingress.

```
kubectl get pods --all-namespaces -l app=ingress-nginx
kubectl get ingress -A
kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx
kubectl get pods -n ingress-nginx
```
![Title](imagens/ingress/ingress2.png)





Verificando o Service:
```
kubectl get svc -n ingress-nginx
```
![Title](imagens/ingress/svc.png)

Note que o meu **ingress-nginx-controller** estÃ¡ como Type **NodePort**. Isso indica que expÃµe o serviÃ§o para fora do cluster via porta do nÃ³ (host). A porta 80 (HTTP) estÃ¡ acessÃ­vel externamente via 32080 e a porta 443 (HTTPS) via 32443.


O Trafego estÃ¡ da seguinte maneira:
```
[ Navegador/Curl ] 
     â†“
http://localhost:32080 (NodePort)
     â†“
[ ingress-nginx-controller Service ]
     â†“
Ingress Controller (NGINX Pod)
     â†“
Ingress Rule
     â†“
Backend Service (ex: giropops-senhas)
     â†“
Pod (aplicaÃ§Ã£o rodando)
```


--------------------








### DNS - Domain Name System

Como meu Cluster foi deployado em uma rede LAN, apontei 3 endereÃ§os DNS para o meu LocalHost, assim consiguirei acessar os DNS de cada ambiente.

vi /etc/hosts
```
127.0.0.1 dev.giropops.local
127.0.0.1 staging.giropops.local
127.0.0.1 prod.giropops.local
```
![Title](imagens/ingress/dns.png)

Nos manifestos values, defini um parametro para declarar o endereÃ§o DNS de cada Ingress.

values-dev.yaml
```
ingress:
  enabled: true
  host: dev.giropops.local # DNS
```
Agora o ingress irÃ¡ apontar para o DNS **dev.giropops.local** que foi definido no arquivos **hosts** do Server.




--------------------















# HPA - Horizontal Pod Autoscaler.

Ã‰ um recurso nativo do kubernetes que ajusta automaticamente o nÃºmero de rÃ©plicas(pods) de um Deployment, ReplicaSet ou StatefulSet com base na utilizaÃ§Ã£o
de recursos ou mÃ©tricas personalizadas.

Exemplo: Utiliza mÃ©tricas definidas em "resource" e "requests" dos containers para escalar.

Para o HPA funcionar, Ã© necessÃ¡rio o Metrics Server instalado no Cluster.


**METRICS SERVER** Ã© um agregador de mÃ©tricas de recursos de sistemas, que coleta mÃ©tricas como uso de CPU e memÃ³ria dos nÃ³s e pods no Cluster.
Essas mÃ©tricas sÃ£o utilizadas no HPA para fazer o escalonamento dos Pods.



Instalando Metrics Server no Kind.
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl patch deployment metrics-server -n kube-system --type=json -p='[
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-insecure-tls"
  },
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-preferred-address-types=InternalIP"
  }
]'

```
![Title](imagens/HPA/install.png)



Verificando instalaÃ§Ã£o:
```
kubectl get pods -n kube-system | grep metrics-server
```
![Title](imagens/HPA/metrics.png)

Verificar se o Metrics Server estÃ¡ rodando corretamente:
```
kubectl get deployment metrics-server -n kube-system
kubectl logs -n kube-system deployment/metrics-server
```

Vamos verificar as mÃ©tricas de CPU dos Nodes:
````
kubectl top nodes
kubectl top pods -n dev
````
![Title](imagens/HPA/top.png)

Metrics Server Instalando e coletando dados.



**Deploy HPA:**

em /templetes criei um manifesto HPA.yaml que declara as especificaÃ§Ãµes de Auto Escaler.

Nos manifestos Values declarei um campo para definir as especificaÃ§Ãµes do HPA. Esses valores sÃ£o carregados automaticamente pelo Helm quando rodo o Update.
```
hpa:
  enabled: true
  minReplicas: 1 #aqui defino o mÃ­nimo de replicas.
  maxReplicas: 3 # aqui defino o mÃ¡ximo de replicas.
  cpuUtilization: 80 #aqui defini o requisito mÃ­nimo de CPU para ativar a regra.
  memoryUtilization: 95 #aqui defini o requisito mÃ­nimo de memÃ³ria RAM para ativar a regra.
  targetDeployment: giropops-senhas # aqui estou apontando para meu deployment.
```
Primeiro busca o valor hpa.targetDeployment do values.yaml. Se nÃ£o estiver definido, cai no helper giropops.fullname.
```
scaleTargetRef:
  name: {{ .Values.hpa.targetDeployment | default (include "giropops.fullname" .) }}
```

Dentro do meu _helpers eu defini o nome da aplicaÃ§Ã£o em vÃ¡riaveis:
```
{{- define "giropops.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{ .Values.fullnameOverride }}
{{- else -}}
{{ .Release.Name }}-{{ .Chart.Name }}
{{- end -}}
{{- end }}
```

Com esta maneira, posso chamar cada values de forma dinÃ¢mica atravÃ©s do:
```
{{ include "giropops.fullname" . }}
```

HPA.yaml
```
{{- if .Values.hpa.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}-hpa
  namespace: {{ .Release.Namespace }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Values.hpa.targetDeployment | default (include "giropops.fullname" .) }}
  minReplicas: {{ .Values.hpa.minReplicas }}
  maxReplicas: {{ .Values.hpa.maxReplicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.hpa.cpuUtilization }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{ .Values.hpa.memoryUtilization }}
{{- end }}
```

Vou atualizar o Helm nos ambientes Staging e Prod para aplicar o HPA.
```
helm upgrade --install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml

helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```



Verificando MÃ©tricas de Prod e Staging:
```
kubectl get hpa -n staging
kubectl get hpa -n prod
```

![Title](imagens/HPA/hpatop.png)







--------------------------------------------



# LOCUST - TESTE DE CARGA 

Locust Ã© uma ferramenta open source escrita em Python para fazer testes de performance e carga.VocÃª escreve scripts de teste em Python que simulam o comportamento de usuÃ¡rios usando sua aplicaÃ§Ã£o.

Como o Locust Funciona?

    VocÃª escreve um script Python descrevendo as aÃ§Ãµes que cada "usuÃ¡rio virtual" deve fazer (ex: logar, acessar pÃ¡gina, fazer post, etc).

    VocÃª executa o Locust â€” que cria milhares de usuÃ¡rios virtuais simulando essas aÃ§Ãµes.

    Ele gera relatÃ³rios interativos em tempo real via Web UI.

    VocÃª analisa latÃªncia, throughput, erros e muito mais.

InstalaÃ§Ã£o:

Em templates/ foi criado um manifesto locust-configmap.yaml com a funÃ§Ã£o de rodar um script(locustfile.py) para simular um teste de carga.

locust-configmap.yaml
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: locust-scripts
data:
  locustfile.py: |
    from locust import HttpUser, task, between

    class GiropopsLoadTest(HttpUser):
        wait_time = between(1, 3)  # Tempo de espera entre requisiÃ§Ãµes

        @task
        def test_homepage(self):
            self.client.get("/")  # Simula requisiÃ§Ãµes para a aplicaÃ§Ã£o

        @task
        def test_generate_password(self):
            self.client.get("/generate")  # Simula geraÃ§Ã£o de senhas
```

locustfile.py
```
from locust import HttpUser, task, between

class Giropops(HttpUser):
    wait_time = between(1, 2)

    @task(1)
    def gerar_senha(self):
        self.client.post("/api/gerar-senha", json={"tamanho": 8, "incluir_numeros": True, "incluir_caracteres_especiais": True})


    @task(2)
    def listar_senha(self):
        self.client.get("/api/senhas")
```

Para Deployment do locust, foi criado um manifesto *locust-deployment.yaml* e um Service *locust-service.yaml*.

locust-deployment.yaml
```
{{- if .Values.locust.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: locust-giropops
  labels:
    app: locust-giropops
spec:
  replicas: {{ .Values.locust.replicas }}
  selector:
    matchLabels:
      app: locust-giropops
  template:
    metadata:
      labels:
        app: locust-giropops
    spec:
      containers:
      - name: locust-giropops
        image: {{ .Values.locust.image }}
        env:
          - name: LOCUST_LOCUSTFILE
            value: "/usr/src/app/scripts/locustfile.py"
        ports:
        - containerPort: 8089
        imagePullPolicy: Always
        volumeMounts:
        - name: locust-scripts
          mountPath: /usr/src/app/scripts
      volumes:
      - name: locust-scripts
        configMap:
          name: {{ .Values.locust.scriptConfigMap }}
          optional: true
{{- end }}
```

locust-service.yaml
```
{{- if .Values.locust.enabled }}
apiVersion: v1
kind: Service
metadata:
  name: locust-service
spec:
  selector:
    app: locust-giropops
  ports:
    - protocol: TCP
      port: 8089
      targetPort: 8089
  type: {{ .Values.locust.service.type }}
{{- end }}
```

Para ativar o Locust nos ambientes, declarei um campo para o Locust marcando como **enable: true** no ambiente de Staging, os demais ambientes deixei o parÃ¢metro como *false*.

values-staging.yaml
```
locust:
  enabled: true  # Habilita ou desabilita o Locust
  image: "linuxtips/locust-giropops:1.0"
  replicas: 1
  service:
    type: NodePort  # Pode ser LoadBalancer na nuvem
    port: 8089
  scriptConfigMap: "locust-scripts"  # Nome do ConfigMap com o locustfile.py
```

### TESTE DE CARGA - LOCUST - HPA

Agora irei verificar o HPA junto com o LOCUST realizando um teste de carga.

No manifesto values-staging.yaml, defini o valor mÃ¡ximo de 10 Pods.
```
hpa:
  enabled: true 
  minReplicas: 1
  maxReplicas: 10
```

Primeiro irei verificar o nome do Pod do Locust e expor via **Port-Foward**.
```
kubectl get pods -n staging -l app=locust-giropops
```
```
kubectl port-forward -n staging pod/locust-giropops-75b9ff794d-dnwbq 8089:8089
```
![Title](imagens/locust/portfowardlocust.png)


Locust acessado, irei simular um teste de carga.

**Teste: a cada segundo, 20 novos usuÃ¡rios comeÃ§am a usar a aplicaÃ§Ã£o atÃ© chegar em 1000 usuÃ¡rios.**

![Title](imagens/locust/locust.png)

![Title](imagens/locust/locustexec.png)


Aqui podemos ver o HPA entrando em aÃ§Ã£o apÃ³s o estresse do Locust, Os Pods existentes bateram a tigger de limite de CPU e MemÃ³ria e comeÃ§aram
a criar novos Pods.

![Title](imagens/locust/podescalando.png)

![Title](imagens/locust/dados.png)





-------------------------




# COSIGN - IMAGENS ASSINADAS E SEGURAS




InstalaÃ§Ã£o Cosign:
```
COSIGN_VERSION=$(curl -s https://api.github.com/repos/sigstore/cosign/releases/latest | grep tag_name | cut -d '"' -f 4)

curl -Lo cosign https://github.com/sigstore/cosign/releases/download/${COSIGN_VERSION}/cosign-linux-amd64

chmod +x cosign

sudo mv cosign /usr/local/bin/
```

```
cosign version
```
![Title](imagens/cosign/version.png)



Cosign instalado, agora irei comeÃ§ar o processo de assinatura de imagem.

Vou gerar um par de chaves:
```
cosign generate-key-pair
```
![Title](imagens/cosign/key.png)

Agora possuo 2 chaves, uma privada e outra pÃºblica.

![Title](imagens/cosign/keys.png)

Assinando imagem:
```
cosign sign --key cosign.key docker.io/geforce8400gsd/giropops-senhas:latest
```

Imagem assinada.
![Title](imagens/cosign/assinado.png)


Verificando assinatura:
```
cosign verify --key cosign.pub docker.io/geforce8400gsd/giropops-senhas:latest
```





# KUBE PROMETHEUS 

Kube Prometheus Ã© uma coleÃ§Ã£o de componentes para instalar e configurar um stack completo de monitoramento no Kubernetes, feito pela comunidade Prometheus + CoreOS.


```
Componente	FunÃ§Ã£o
Prometheus	Coleta e armazena mÃ©tricas (CPU, memÃ³ria, requests, etc.)
Grafana	Interface para dashboards lindÃµes ðŸŽ¨
Alertmanager	Envia alertas (Slack, email, etc)
Node Exporter	Exporta mÃ©tricas do nÃ³ (CPU/disk/etc.)
kube-state-metrics	MÃ©tricas do estado dos recursos K8s
Prometheus Operator	Facilita deploys de Prometheus via CRDs
```


Iniciando a instalaÃ§Ã£o Kube Prometheus, irei adicionar as CDR (Custom Resource Definition) no HELM:
````
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
````
Criando Namespace:
````
kubectl create namespace monitoring
````
Agora irei iniciar a instalaÃ§Ã£o do *kube prometheus stack* na namespace *monitoring*.
```
helm install kube-prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set grafana.enabled=true \
  --set prometheus.resources.limits.cpu=100m \
  --set prometheus.resources.limits.memory=128Mi \
  --set grafana.resources.limits.cpu=100m \
  --set grafana.resources.limits.memory=128Mi
```

Verificando a instalaÃ§Ã£o:
````
kubectl get pods -n monitoring
kubectl get svc -n monitoring
````

![Title](imagens/monitoring/install.png)


![Title](imagens/monitoring/svcpod.png)

Pods e Services rodando com sucesso, agora irei criar o **Service Monitor**.

### Service Monitor

Service monitor Ã© uma Custom Resource Definition (CRD) usado pelo Prometheus Operator no kubernetes...

Ele jÃ¡ vem instalado no kube-prometheus. O Kube-prometheus jÃ¡ vem com vÃ¡rios ServiceMonitors configurados.
Para visualizar os servicemonitors:
````
kubectl get servicemonitors -n monitoring
````

Criei um manifesto para o service monitor para monitorar o ambiente de ProduÃ§Ã£o.
servicemonitor-prod.yaml
````
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: giropops-senhas-servicemonitor
  namespace: monitoring  # mesmo namespace do Prometheus
  labels:
    release: kube-prometheus  # importante se Prometheus usa labelSelector
spec:
  selector:
    matchLabels:
      app: giropops-senhas  # deve bater com o .Values.services.giropops-senhas.labels.app
  namespaceSelector:
    matchNames:
      - dev  # ou "prod", dependendo de onde estÃ¡ seu app
  endpoints:
    - port: giropops-senhas-metrics  # mesmo nome usado no service
      path: /metrics                 # endpoint exposto no app
      interval: 15s
````

Aplicando o manifest:
```
kubectl apply -f servicemonitor-prod.yaml
```


### Nginx Ingress e DNS - EXPONDO GRAFANA E PROMETHEUS

Para expor o Grafana e Prometheus, criei 2 manifestos ingress que apontam para os services *kube-prometheus-grafana* e *kube-prometheus-kube-prome-prometheus*.

ingress-grafana.yaml
````
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: grafana.giropops.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-grafana
                port:
                  number: 80
````
ingress-prometheus.yaml
````
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: prometheus.giropops.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-prometheus
                port:
                  number: 9090

````

Para o DNS, adicionei *prometheus.giropops.local* e *grafana.giropops.local* no arquivo **/etc/hosts**.
![Title](imagens/monitoring/dnsprome.png)


Aplicando Ingress:
````
kubectl apply -f ingress-prometheus.yaml
kubectl apply -f grafana.yaml
````

Prometheus e Grafana acessados com sucesso.
![Title](imagens/monitoring/prografana.png)










