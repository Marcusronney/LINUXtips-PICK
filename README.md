# Projeto final do Programa Intensivo em Containers e Kubernetes | PICK LINUXtips 

O Projeto final consite na resolu√ß√£o do desafio PICK 2024_01 proposto pelo [@Badtuxx](https://github.com/badtuxx). O desafio consiste em um projeto pr√°tico de DevOps focado na aplica√ß√£o de gest√£o de senhas [Giropops-Senhas](https://github.com/badtuxx/giropops-senhas). O projeto exige Stacks com ferramentas de containeriza√ß√£o, orquestra√ß√£o, seguran√ßa e monitoramento, com foco em ambientes seguros, CI/CD,  automatizados e observ√°veis com alta disponibilidade.

**Objetivos principais:**

  Containerizar a aplica√ß√£o com Docker e public√°-la.

  Orquestrar o deployment com Kubernetes e expor o servi√ßo com Ingress.

  Automatizar o deploy utilizando Helm Charts.

  Implementar seguran√ßa de runtime com Kyverno e Cosign (assinatura de imagens).

  Monitorar m√©tricas com Prometheus e configurar alertas.

  Distribuir pacotes da aplica√ß√£o utilizando Melange e Apko.

  Implementar CI/CD para build e publica√ß√£o dos pacotes em m√∫ltiplos ambientes

### Tecnologias envolvidas no projeto:

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white&style=for-the-badge)
![Redis](https://img.shields.io/badge/Redis-DB%20Cache-red?logo=redis&logoColor=white&style=for-the-badge)
![Flask](https://img.shields.io/badge/Flask-Web%20Framework-black?logo=flask&logoColor=white&style=for-the-badge)
![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI%2FCD-2088FF?logo=githubactions&logoColor=white&style=for-the-badge)
![YAML](https://img.shields.io/badge/YAML-Configuration-cccccc?logo=yaml&logoColor=white&style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-Container-2496ED?logo=docker&logoColor=white&style=for-the-badge)
![Kubernetes](https://img.shields.io/badge/Kubernetes-Orchestration-326CE5?logo=kubernetes&logoColor=white&style=for-the-badge)
![Grafana](https://img.shields.io/badge/Grafana-Analytics-F46800?logo=grafana&logoColor=white&style=for-the-badge)
![Prometheus](https://img.shields.io/badge/Prometheus-Monitoring-E6522C?logo=prometheus&logoColor=white&style=for-the-badge)
![Linux](https://img.shields.io/badge/Linux-OS-FCC624?logo=linux&logoColor=black&style=for-the-badge)
![Locust](https://img.shields.io/badge/Locust-Load%20Testing-0FA36B?logo=locust&logoColor=white&style=for-the-badge)
![Melange](https://img.shields.io/badge/Melange-APK%20Builder-5e5e5e?style=for-the-badge)
![Apko](https://img.shields.io/badge/Apko-Container%20Images-5e5e5e?style=for-the-badge)
![Helm](https://img.shields.io/badge/Helm-Package%20Manager-0F1689?logo=helm&logoColor=white&style=for-the-badge)
![Kyverno](https://img.shields.io/badge/Kyverno-Policy%20Engine-4B8BBE?style=for-the-badge)
![Cosign](https://img.shields.io/badge/Cosign-Image%20Signing-00ADD8?style=for-the-badge)
![Kind](https://img.shields.io/badge/Kind-Kubernetes%20Cluster-3DDC84?style=for-the-badge)
![Trivy](https://img.shields.io/badge/Trivy-Vulnerability%20Scanner-20C997?style=for-the-badge)
![NGINX Ingress](https://img.shields.io/badge/Nginx%20Ingress-Load%20Balancer-009639?logo=nginx&logoColor=white&style=for-the-badge)
![DNS](https://img.shields.io/badge/DNS-Domain%20Name%20System-5e5e5e?style=for-the-badge)
![HPA](https://img.shields.io/badge/HPA-Auto%20Scaling-326CE5?logo=kubernetes&logoColor=white&style=for-the-badge)
![Kube-Prometheus](https://img.shields.io/badge/Kube--Prometheus-Monitoring-326CE5?logo=kubernetes&logoColor=white&style=for-the-badge)
![Chainguard](https://img.shields.io/badge/Chainguard-Supply%20Chain%20Security-5e5e5e?style=for-the-badge)
![Docker Hub](https://img.shields.io/badge/Docker_Hub-Repository-2496ED?logo=docker&logoColor=white&style=for-the-badge)
![Hyper-V](https://img.shields.io/badge/Hyper--V-Virtualization-0078D7?style=for-the-badge)

------------------------------------


## INFRAESTRUTURA ON PREMISE

No meu projeto, o Cluster ir√° rodar localmente em uma VM no Hyper-V.

# Especifica√ß√µes:

|       VM       |       CPU        |     RAM     |     OS      |  CLUSTER KUBERNETES  |
|----------------|------------------|-------------|-------------|----------------------|
|     Hyper-V    | Xeon 2360 4 Core |     17GB    | Rocky Linux |         KinD         |


# PROVISIONANDO SERVER PARA O CLUSTER.

**Rede**:

Irei configurar o IP 192.168.1.81/24 de forma est√°tica na subrede 192.168.1.x junto com meu gateway, o DNS ser√° do Google (8.8.8.8).
```
nmcli con mod eth0 ipv4.addresses 192.168.1.81/24
nmcli con mod eth0 ipv4.gateway 192.168.1.254
nmcli con mod eth0 ipv4.dns 8.8.8.8
nmcli con mod eth0 ipv4.method manual
```

Reiniciando a conex√£o para aplicar as altera√ß√µes.
````
nmcli con down eth0 && nmcli con up eth0
````

Visualizando:

*ip a show eth0
nmcli dev show eth0*


Rede Configurada. -----------------------------------------------------------------


Atualizando o OS.
```
dnf update -y
```
Instale o Git:
```
sudo dnf install -y git
```


# VM atualizada e Git instalado, agora vamos para o Docker.

![Linux](https://img.icons8.com/?size=100&id=gE2kWztHPCpb&format=png&color=000000)

### Docker
Docker √© uma plataforma que facilita criar, empacotar e rodar aplica√ß√µes dentro de containers. Um container √© um ambiente leve, port√°til e isolado que roda uma aplica√ß√£o e suas depend√™ncias (bibliotecas, arquivos de configura√ß√£o, etc.), tudo empacotado junto.

```
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
```
### Conterinerd
Containerd √© o runtime de containers ‚Äî ou seja, √© o motor que roda containers 
```
sudo dnf install -y docker-ce docker-ce-cli containerd.io
```

Habilitei o service do docker na inicializa√ß√£o.
```
sudo systemctl enable docker --now
```
Verificando a instala√ß√£o
````
docker --version
````
![Title](imagens/docker/docker_instalado.png)



### Kubectl
kubectl √© a ferramenta de linha de comando usada para interagir com um cluster Kubernetes. Quando voc√™ executa um comando, ele envia uma requisi√ß√£o para a API Server, que coordena as a√ß√µes necess√°rias no cluster.


Instalando a vers√£o stable do kubectl, dando permiss√£o para execu√ß√£o e movendo a sa√≠da para o diret√≥rio */usr/local/bin/*
```
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
```

Verifique a instala√ß√£o
````
kubectl version --client
````
![Title](imagens/kubectl/kubectl.png)



# KinD - Kubernetes in Docker
Kind √© uma ferramenta que permite criar clusters Kubernetes locais usando containers Docker como n√≥s do cluster.

Baixaxando o kind, dando permiss√£o de execu√ß√£o e movendo para o diret√≥rio */usr/local/bin/kind*
```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

Verificando a instala√ß√£o:
````
kind --version
````
![Title](imagens/kind/kind.png)


Kind instalado com sucesso. Irei criar um manifest para deploy do Cluster.


kind-config.yaml
```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  apiServerAddress: "192.168.1.81"  # IP externo do host
  apiServerPort: 17443
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"

nodes:
  - role: control-plane
    extraPortMappings:
      - containerPort: 80
        hostPort: 80
      - containerPort: 443
        hostPort: 443
      - containerPort: 6443
        hostPort: 17445  # Porta do servidor API

  - role: worker
```



# Deploy:
```
kind create cluster --name giropops-cluster --config cluster-config.yaml
```

![Title](imagens/kind/1.png)



Vamos verificar se o Cluster subiu corretamente.

Cluster:
````
kubectl cluster-info --context kind-giropops-cluster
````
Nodes:
````
kubectl get nodes
````
Se tudo estiver OK, voc√™ ver√°:

    O endere√ßo do API server https://192.168.1.81:17443

    Dois nodes: control-plane e worker com STATUS: Ready


![Title](imagens/kind/2.png)

Cluster funcionando perfeitamente.


**Preparando a imagem Docker para Deploy**






# DOCKER ![Docker](https://img.icons8.com/?size=100&id=22813&format=png&color=000000)

Docker √© uma plataforma open-source que permite empacotar uma aplica√ß√£o e todas as suas depend√™ncias em um container ‚Äî um ambiente isolado que roda de forma consistente em qualquer.
Basicamente ele tem a fun√ß√£o de mentir para a aplica√ß√£o para que ela pense que est√° rodando em uma m√°quina com hardware independente, quando na realidade todos os recursos est√£o isolados.


Irei criar um Dockerfile Single-Stage Runtime para a aplica√ß√£o.


Vamos criar um **Dockerfile** dentro do diret√≥rio da aplica√ß√£o.

***Estrutura para o Dockerfile:***

giropops-senhas/

‚îú‚îÄ‚îÄ app.py

‚îú‚îÄ‚îÄ requirements.txt

‚îú‚îÄ‚îÄ templates/

‚îú‚îÄ‚îÄ static/

‚îî‚îÄ‚îÄ Dockerfile




**Dockerfile**

```
FROM python:3.12-slim #estamos definindo a imagem oficinal Python

WORKDIR /app #Aqui definimos o diret√≥rio principal do container, todos os comandos agora ir√£o rodar a parti do /app

COPY requirements.txt . #/app/requirements.txt
COPY app.py . #/app/app.py
COPY templates templates/ #/app/templates/
COPY static static/ #/app/static/

RUN pip install --no-cache-dir -r requirements.txt #aqui, vamos instalar as depend√™ncias dentro do .txt

EXPOSE 5000 #o container ir√° escutar na porta 5000

CMD ["flask", "run", "--host=0.0.0.0"] #definimos um comando para subir o flask e aceitar conex√£o de qualquer ip.

```


Buildando a aplica√ß√£o.

```
docker build -t giropops-senhas .
```

Agora irei expor o container na porta 5000 e testar a aplica√ß√£o.
```
docker run -p 5000:5000 giropops-senhas
```

![Title](imagens/docker/buildgiro.png)


Aplica√ß√£o funcionando.

![Title](imagens/docker/pops.png)

Agora que criamos e buildamos a aplica√ß√£o giropops-senhas no Docker, vamos analisar a imagem.

Usando o comando **docker images** e **docker history** conseguimos visualizar a imagem criada e o seu processo de build. Note que a imagem possu√≠ um tamanho de 140MB.

![Title](imagens/docker/dockerimage.png)

Sa√≠da do docker history:
```
IMAGE          CREATED         CREATED BY                                 SIZE      COMMENT
527daaa695dc   11 minutes ago  CMD ["flask" "run" "--host=0.0.0.0"]        0B
<missing>      11 minutes ago  EXPOSE map[5000/tcp:{}]                    0B
<missing>      11 minutes ago  RUN pip install --no-cache-dir ...         15.3MB
<missing>      11 minutes ago  COPY static static/                        101kB
<missing>      11 minutes ago  COPY templates templates/                  5.78kB
<missing>      11 minutes ago  COPY app.py .                              5.59kB
<missing>      1 hour ago      COPY requirements.txt .                    51B
<missing>      1 hour ago      WORKDIR /app                               0B
<missing>      7 days ago      FROM python:3.12-slim                      74.8MB (base image + depend√™ncias)
```
![Title](imagens/docker/dockerhistory.png)

Aqui podemos visualizar todas as camadas do processo de build da nossa aplica√ß√£o. Uma imagem Single-Stage, tamanho um pouco elevado e com uma superf√≠cie de ataque elevada.
Uma imagem Single-Stage √© o modelo mais simples de constru√ß√£o de imagens Docker, onde todo o processo de build acontece em um √∫nico Dockerfile.



# Agora, irei realizar o build da aplica√ß√£o utilizando **Melange** + **APKO**.



# MELANGE

O Melange √© uma ferramenta para construir pacotes para sistemas baseados em 
Alpine Linux e APKO atrav√©s de manifestos YAML. Ele permite que voc√™ crie pacotes .apk que podem ser inclu√≠dos em imagens APKO e 
usados em containers leves. APKO pega pacotes .apk (como os que o Melange criou) e monta uma imagem de container OCI ‚Äî segura, minimalista e sem precisar de Dockerfile nem daemon Docker.


Instalando o Melange.
```
curl -L https://github.com/chainguard-dev/melange/releases/download/v0.23.6/melange_0.23.6_linux_amd64.tar.gz -o melange.tar.gz
tar -xzf melange.tar.gz
```
Arquivo extra√≠do, irei entrar no diret√≥rio, dar permiss√£o e mover para /ur/local/bin/melange
````
cd melange_0.23.6_linux_amd64/

#ls
LICENSE  melange

mv melange /usr/local/bin/
chmod +x /usr/local/bin/melange
````
![Title](imagens/melange/melange.png)


Testando o Melange:
````
melange version
````

![Title](imagens/melange/1.png)


# APKO

APKO √© uma ferramenta mantida epla Chainguard que tem a fun√ß√£o de criar imagens de containers seguras, minimalistas e super otimizadas, apenas declarando o que voc√™ quer num arquivo de configura√ß√£o YAML a parti de imagens Alpine Linux, sem precisar usar um Dockerfile.

Instala√ß√£o:
```
curl -L https://github.com/chainguard-dev/apko/releases/download/v0.10.0/apko_0.10.0_linux_amd64.tar.gz -o apko.tar.gz
tar -xzf apko.tar.gz
cd apko_0.10.0_linux_amd64/
chmod +x apko
sudo mv apko /usr/local/bin/
```

![Title](imagens/melange/apko.png)


Verificando a vers√£o:
````
apko version
````

![Title](imagens/melange/apko1.png)



**MELANGE e APKO Instalado com sucesso!**

Criando pacote de imagem com Melange:

Gerando as chaves:
```
melange keygen
```

![Title](imagens/melange/3.png)

Agora possuo 2 chaves, uma privada "melange.rsa" e outra p√∫blica "melange.rsa.pub".


Renomeando a chave p√∫blica:
```
mkdir keys
cp melange.rsa.pub melange.key
```





Declarando as especifica√ß√µes da imagem via manifesto melange.yaml

vi melange.yaml

```
package:
  name: giropops-senhas
  version: 0.1
  description: Aplica√ß√£o Giropops-senhas - Gerador de senhas - LinuxTips
  dependencies:
    runtime:
      - python3

environment:
  contents:
    keyring:
      - ./melange.rsa.pub
    repositories:
      - https://dl-cdn.alpinelinux.org/alpine/edge/main
      - https://dl-cdn.alpinelinux.org/alpine/edge/community
    packages:
      - alpine-baselayout-data
      - ca-certificates-bundle
      - busybox
      - gcc
      - musl-dev
      - python3
      - python3-dev
      - py3-pip
      - py3-virtualenv
pipeline:
  - name: Build Python application
    runs: |
      EXECDIR="${{targets.destdir}}/usr/bin"
      WEBAPPDIR="${{targets.destdir}}/usr/share/webapps/giropops-senhas"
      mkdir -p "${EXECDIR}" "${WEBAPPDIR}"
      echo "#!/usr/share/webapps/giropops-senhas/venv/bin/python3" > "${EXECDIR}/giropops-senhas"
      cat app.py >> "${EXECDIR}/giropops-senhas"
      chmod +x "${EXECDIR}/giropops-senhas"
      virtualenv "${WEBAPPDIR}/venv"
      cp -r templates/ static/ ${WEBAPPDIR}/
      sh -c "source '${WEBAPPDIR}/venv/bin/activate' && pip install -r requirements.txt"
```


Resumo do manifesto melange:

    Define um pacote chamado giropops-senhas, vers√£o 0.1.

    Declara depend√™ncias do sistema necess√°rias para compilar e rodar uma app Python (gcc, musl, python3, etc).

    Cria uma pipeline de build, que:

        Prepara diret√≥rios destino.

        Copia e monta o script app.py como um execut√°vel (giropops-senhas).

        Cria um ambiente virtual Python (venv) no diret√≥rio de webapp.

        Copia pastas templates/ e static/.

        Instala as depend√™ncias do requirements.txt.

    
Agora, irei criar o manifesto do apko.

vi apko.yaml
```
contents:
  repositories:
    - https://dl-cdn.alpinelinux.org/alpine/edge/main
    - /work/packages
  packages:
    - alpine-baselayout
    - giropops-senhas
    - curl

accounts:
  groups:
    - groupname: nonroot
      gid: 65532
  users:
    - username: nonroot
      uid: 65532
      gid: 65532
  run-as: 65532

environment: 
  FLASK_APP: "/usr/bin/giropops-senhas"

entrypoint:
  command: /usr/bin/giropops-senhas

archs:
  - x86_64
```

Criando o diret√≥rio mkdir packages/
````
mkdir packages
````
Estrutura:
giropops-senhas/
‚îú‚îÄ‚îÄ app/                        # C√≥digo-fonte da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îú‚îÄ‚îÄ melange.yaml         # Receita do pacote .apk
‚îú‚îÄ‚îÄ apko.yaml            # Receita da imagem
‚îú‚îÄ‚îÄ melange.key          # Chave p√∫blica
‚îú‚îÄ‚îÄ melange.rsa          # Chave privada
|   melange.rsa.pub      # Chave p√∫blica
‚îî‚îÄ‚îÄ packages/            # Onde o .apk ser√° salvo
‚îú‚îÄ‚îÄ output/              # Gerado automaticamente com os pacotes
    ‚îî‚îÄ‚îÄ packages/x86_64/



**Buildando a imagem**.



Empacotando imagem com **Melange** via Docker.
```
docker run --rm --privileged \
  -v "${PWD}:/work" \
  -w /work \
  cgr.dev/chainguard/melange \
  build melange.yaml \
  --arch x86_64 \
  --signing-key ./keys/melange.rsa \
  --repository-append ./packages
```

Os campos **INFO wrote packages/x86_64/giropops-senhas-0.1-r0.apk** e 
**INFO writing signed index to packages/x86_64/APKINDEX.tar.gz** indicam que o empacotamento foi realizado com sucesso.

![Title](imagens/melange/buildmelange.png)


Agora possuo .apk da aplica√ß√£o.

√åndice assinado e o diret√≥rio **packages/x86_64/** pronto para o apko.
```
#ls packages/x86_64/

APKINDEX.tar.gz  giropops-senhas-0.1-r0.apk
```



Realizando Build com **APKO:**

```
docker run --rm \
  -v "${PWD}:/work" \
  -w /work \
  cgr.dev/chainguard/apko \
  build apko.yaml giropops-senhas giropops.tar \
  -k melange.rsa.pub
```

apko.yaml ‚Üí especifica o que vai na imagem (pacotes, entrypoint etc.)

giropops-senhas ‚Üí nome l√≥gico da imagem

giropops.tar ‚Üí imagem gerada como tarball

-k melange.rsa.pub ‚Üí chave p√∫blica usada para verifica√ß√£o de pacotes

**Imagem APKO Constru√≠da com sucesso.**

![Title](imagens/melange/5.png)


Verificando o arquivo gerado: 
```
ls -lh giropops.tar
```
![Title](imagens/melange/4.png)



# SUBINDO IMAGEM APKO para o DOCKER HUB.

Irei carregar a imagem.tar para o reposit√≥rio local:
````
docker load < giropops.tar
````

![Title](imagens/melange/apko_docker.png)

Realizando um teste da imagem empacotada via APKO:

```
docker run -p 5000:5000 giropops-senhas:latest-amd64
```

# Build Melange e empacotamento APKO realizado com sucesso!

![Title](imagens/melange/buildapko.png)



# Docker HUB

Irei realizar login no Docker Hub, definir uma tag para a imagem criada e fazer push.
```
docker login
docker tag
docker push
```




Imagem APKO upada no Docker Hub com apenas **25.75 MB**.

![Title](imagens/melange/dockerhub.png)




# TRIVY - An√°lise de Vulnerabilidades

O Trivy √© uma ferramenta de c√≥digo aberto que serve como scanner de vulnerabilidades, ideal para identificar falhas em imagens de cont√™ineres.

![Linux](https://img.icons8.com/?size=100&id=UjcGNVXknmz3&format=png&color=000000)

Instalando:
```
curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin

aquasecurity/trivy info checking GitHub for latest tag
aquasecurity/trivy info found version: 0.61.0 for v0.61.0/Linux/64bit
aquasecurity/trivy info installed /usr/local/bin/trivy
```
![Title](imagens/trivy/trivy.png)


Vamos verificar as vulnerabilidades da nossa imagem AKPO.
```
trivy image --severity HIGH,CRITICAL --ignore-unfixed geforce8400gsd/giropops-senhas
```

![Title](imagens/trivy/scan1.png)


**0 Vulnerabilidades**.

Agora, vamos comparar as vulnerabilidades da nossa imagem APKO com o nosso Dockerfile.


![Title](imagens/trivy/trivy_giropops.png)

**105 Vulnerabilidades**.


### Comparativo de Imagens - AKPO vs Docker

Agora possuo uma imagem Ultra-minimalista com apenas 20 MB e com uma superf√≠cie de ataques reduzida, possuindo apenas o necess√°rio. Note que diferen√ßa de vulnerablidades, de 105 para 0.

| Crit√©rio                   | Dockerfile Cl√°ssico (`python:3.12-slim`) | Melange + APKO (`alpine`)           |
|---------------------------|-------------------------------------------|-------------------------------------|
| üì¶ Imagem base            | python:3.12-slim (~74MB)                  | alpine (~5MB base)                  |
| üìê Tamanho final da imagem| ~140MB                                    | ~20-25MB                            |
| üõ°Ô∏è Seguran√ßa               | Usu√°rio root                              | Usu√°rio **n√£o-root** (UID 65532)    |
| üßº Imagem limpa           | Cont√©m pip, gcc, cache, etc.              | S√≥ o necess√°rio, nada de build tools|
| üîÅ Reprodutibilidade      | Parcial                                   | **Total (com assinatura RSA)**      |
| üîê Supply Chain Security  | N√£o possui verifica√ß√£o de pacotes         | **Melange + assinatura de pacotes** |
| üîß Complexidade           | Baixa (f√°cil de aprender)                 | Moderada (curva de aprendizado maior)|
| Vulnerabilidades           | 105                                       | 0                                    |

------------------------------------------------------------------


# Kubernetes

Kubernetes (tamb√©m chamado de K8s) √© uma plataforma open-source de orquestra√ß√£o de containers.
Ele automatiza o deploy, o scaling e a gest√£o de aplica√ß√µes containerizadas.

![Kubernetes](https://img.icons8.com/?size=100&id=cvzmaEA4kC0o&format=png&color=000000)

Arquitetura do Kubernetes:
```
Componente	Fun√ß√£o
Cluster	Conjunto de m√°quinas (nodes) onde os containers rodam
Master Node	Onde roda o ‚Äúc√©rebro‚Äù do Kubernetes (API, scheduler, controller)
Worker Nodes	Onde os containers s√£o executados
Pod	A menor unidade de execu√ß√£o: 1 ou mais containers agrupados
Deployment	Gerencia a cria√ß√£o/atualiza√ß√£o de m√∫ltiplos pods
Service	Exposi√ß√£o de pods por IP fixo / load balancing
Ingress	Regras de roteamento HTTP/HTTPS externas
ConfigMap / Secret	Inje√ß√£o de configs e dados sens√≠veis
Namespace	Isolamento l√≥gico entre aplica√ß√µes
```

**Buildando a imagem no Kubernetes com KinD.**

KinD = ‚ÄúKubernetes IN Docker‚Äù
√â uma forma super leve de rodar um cluster Kubernetes completo dentro de containers Docker.


Como o KiND j√° est√° instalado atrav√©s do manifesto kind-config.yaml, irei prosseguir para o build da aplica√ß√£o. 

Verificando o Cluster.

O KinD √© um Kubernetes no qual executa o cluster via containers com Docker, irei verificar os Containers respons√°veis para o cluster k8s.

Verificando os containers Kind:
```
docker ps -a
```
![Title](imagens/kind/dockerps-a.png)

Container **giropops-cluster-worker** e **giropops-cluster-control-plane** rodando perfeitamente.




Verificando o Cluster:
```
kind get clusters
```
![Title](imagens/kind/dockerps-a.png)


Verificando informa√ß√µes do Cluster e os Nodes
```
kubectl cluster-info
kubectl get nodes
```

![Title](imagens/kind/nodes_info.png)

Por √∫ltimo, Pods do Kubernetes:

```
kubectl get pods -A
```

![Title](imagens/kind/kubectlgetpods-A.png)

Agora podemos visualizar a menor unidade do nosso Cluster Kubernetes, os PODs, em resumo:

coredns (2 pods)	DNS interno do cluster

etcd	Banco de dados do cluster

kube-apiserver	API principal do Kubernetes

kube-controller-manager	Controlador de recursos

kube-scheduler	Agenda pods nos nodes

kube-proxy (2 pods)	Regras de rede por node

kindnet (2 pods)	CNI de rede do KinD

Conseguimos visualizar a sa√∫de do nosso Cluster, todos os Pods necess√°rios para o cluster funcionar est√£o rodando perfeitamente na namespace kube-system.



----------------------------

Buildando a imagem Kubernetes.

Irei realizar o build e teste da aplica√ß√£o.

Manifestos:

giropops-deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: giropops-senhas
  name: giropops-senhas
spec:
  replicas: 1
  selector:
    matchLabels:
      app: giropops-senhas
  template:
    metadata:
      labels:
        app: giropops-senhas
    spec:
      containers:
      - image: geforce8400gsd/giropops-senhas:latest
        name: giropops-senhas
        env:
        - name: REDIS_HOST
          value: redis-service
        ports:
        - containerPort: 5000
```

giropops-service.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: giropops-senhas-service
spec:
  type: NodePort
  selector:
    app: giropops-senhas
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
    nodePort: 32000  # Escolha uma porta diferente
```

redis-deployment.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379
  type: ClusterIP
```

redis-service.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: redis
  name: redis-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - image: redis
        name: redis
        ports:
          - containerPort: 6379
        resources:
          limits:
            memory: "256Mi"
            cpu: "500m"
          requests:
            memory: "128Mi"
            cpu: "250m"
```

Build:
```
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl apply -f redis-deployment.yaml
kubectl apply -f redis.yaml
```

Manifestos yaml buildados, vamos visualizar os Pods e Services.

```
kubectl get pods --all-namespaces
```
```
kubectl get svc --all-namespaces
```
Agora podemos ver os Pods e Services da aplica√ß√£o e do Redis, irei expor via *port-foward* e testar o acesso da aplica√ß√£o.

![Title](imagens/kubernetes/kubectl.png)


Port-Foward:
```
kubectl port-forward deployment/giropops-senhas 5000:5000
```
![Title](imagens/kubernetes/port-foward.png)


**Aplica√ß√£o Buildada e acessada no Kubernetes com sucesso.**

![Title](imagens/kubernetes/aplicacao_kubernetes.png)






# HELM - Gest√£o de Pacotes do Kubernetes

O Helm √© uma ferramenta open-source que permite gerenciar aplica√ß√µes Kubernetes de forma simples e eficiente. Com o Helm, voc√™ pode instalar, atualizar e desinstalar aplica√ß√µes em um cluster Kubernetes com facilidade.



**Instalando HELM:**
```
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```
Verifique a vers√£o:
```
HELM version
```
![Title](imagens/helm/helminstall.png)


------

**Estruturando Cluster com HELM**

Definindo a estrutura do Projeto.
```
giropops-senhas/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml (base)
‚îú‚îÄ‚îÄ values-dev.yaml
‚îú‚îÄ‚îÄ values-prod.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deploymentlocust.yaml
‚îÇ   ‚îú‚îÄ‚îÄ redis-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ redis-service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service-locust.yaml
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml
‚îÇ   ‚îú‚îÄ‚îÄ locust-configmap.yaml
‚îÇ   ‚îî‚îÄ‚îÄ _helpers.tpl
```

No projeto, um dos requisitos √© ter 3 ambientes, um de produ√ßao, outro de teste e outro de desenvolvimento. A separa√ß√£o dos ambientes ser√° feita atrav√©s dos manifestos values, exemplo: values-prod.yaml, values-dev.yaml e values-staging.yaml.

Criando as Namespaces:
```
kubectl create namespace dev
kubectl create namespace staging
kubectl create namespace prod
```
![Title](imagens/kubernetes/ns.png)


**Deploy Helm:** values-dev.yaml, values-staging.yaml e values-prod.yaml

Dev:
```
helm upgrade --install giropops-dev . \
  --namespace dev \
  --values values-dev.yaml
```
Staging:
```
helm install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml 
```
Prod:
```
helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```

![Title](imagens/helm/deploy.png)


Agora o cluster possu√≠ as Namespaces dos 3 ambientes, prod, dev e staging.
```
helm list
```
![Title](imagens/helm/helmlist.png)


Verificando os pods:
```
kubectl get pods -n dev
kubectl get pods -n staging
kubectl get pods -n prod
```
![Title](imagens/helm/helmlistpod.png)


Vamos verificar os pods criados:
```
kubectl get all --all-namespaces
```
![Title](imagens/helm/all.png)


**Um detalhe**, no values-dev.yaml defini **hpa: enabled: false** para que o HPA funcione apenas nos ambientes prod e staing.

![Title](imagens/helm/hpa.png)











# NGINX INGRESS - Expondo o Cluster

![Nginx](https://img.icons8.com/?size=100&id=f8puwbhs0kUR&format=png&color=000000)

Ingress √© um recurso do Kubernetes que gerencia o acesso externo de um servi√ßo dentro do Cluster.

Ele funciona como uma camada de Roteamento HTTP/HTTPS, permitindo a defini√ß√£o de regras para direcionar
o tr√°fego externo para diferentes servi√ßos back-end.

Como ele ir√° funcionar?
```
Navegador ‚Üì
http://giropops.local:32080
     ‚Üì
Ingress Controller (NGINX)
     ‚Üì
Ingress Rule (roteamento)
     ‚Üì
Service (ex: giropops-senhas)
     ‚Üì
Pod (sua aplica√ß√£o Flask ou FastAPI ou NodeJS etc)

```

Instala√ß√£o Nginx Ingress:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/kind/deploy.yaml
```


Verificando os Pods Nginx.

O NGINX Controller, por padr√£o, n√£o especifica um *nodeSelector*, mas √†s vezes os *taints/tolerations* ou a falta de afinidade impedem ele de ser escalonado nesse √∫nico node.
Se o pod n√£o iniciar, rode:
```
kubectl patch deployment ingress-nginx-controller -n ingress-nginx \
  --type='json' -p='[{
    "op": "add",
    "path": "/spec/template/spec/nodeSelector",
    "value": {
      "kubernetes.io/hostname": "giropops-cluster-control-plane"
    }
  }]'
```


```
kubectl get pods -n ingress-nginx
```
![Title](imagens/ingress/ingressnginx.png)

Realizando o Deploy do Ingress NGINX com as portas 32080 (HTTP) e 32443 (HTTPS)


Agora irei realizar deploy do manifestos ingress.yaml.

ingress.yaml
```
{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: giropops-ingress
  namespace: {{ .Release.Namespace }}
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    {{- if .Values.ingress.allowIpAccess }}
    nginx.ingress.kubernetes.io/whitelist-source-range: "192.168.1.0/24"
    {{- end }}
spec:
  ingressClassName: nginx
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ .Values.ingress.serviceName }}
                port:
                  number: {{ .Values.ingress.servicePort }}
{{- end }}
```

Para expor todos os Cluster com 1 manifesto Ingress foi definido as especifica√ß√µes via v√°riaveis.

No Manifesto ingress defini v√°riaveis ir√° buscar o valor em cada 1 dos values.
Exemplo Ingress.yaml:
```
name: {{ include "giropops.fullname" . }}-ingress
service:
                name: {{ .Values.ingress.serviceName }}
                port:
                  number: {{ .Values.ingress.servicePort }}
```
values-dev.yaml:
```
ingress:
  enabled: true
  host: dev.giropops.local
  serviceName: giropops-senhas-giropops-senhas-port
  servicePort: 5000
  allowIpAccess: true
```




Agora, irei fazer upgrade do HELM para atualizar os 3 Ambientes prod, staging e dev:
```
helm upgrade --install giropops-dev . \
  --namespace dev \
  --values values-dev.yaml

helm upgrade --install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml

helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```

Verificando os ingress criados:
```
kubectl get ingress -n dev
kubectl get ingress -n prod
kubectl get ingress -n staging
```
![Title](imagens/ingress/ingress-n.png)


Verificando os Pods Nginx Ingress.

```
kubectl get pods --all-namespaces -l app=ingress-nginx
kubectl get ingress -A
kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx
kubectl get pods -n ingress-nginx
```
![Title](imagens/ingress/ingress2.png)





Verificando o Service:
```
kubectl get svc -n ingress-nginx
```
![Title](imagens/ingress/svc.png)

Note que o meu **ingress-nginx-controller** est√° como Type **NodePort**. Isso indica que exp√µe o servi√ßo para fora do cluster via porta do n√≥ (host). A porta 80 (HTTP) est√° acess√≠vel externamente via 32080 e a porta 443 (HTTPS) via 32443.


O Trafego est√° da seguinte maneira:
```
[ Navegador/Curl ] 
     ‚Üì
http://localhost:32080 (NodePort)
     ‚Üì
[ ingress-nginx-controller Service ]
     ‚Üì
Ingress Controller (NGINX Pod)
     ‚Üì
Ingress Rule
     ‚Üì
Backend Service (ex: giropops-senhas)
     ‚Üì
Pod (aplica√ß√£o rodando)
```


--------------------








### DNS - Domain Name System

Como meu Cluster foi deployado em uma rede LAN, apontei 3 endere√ßos DNS para o meu Localhost, assim conseguirei acessar os DNS de cada ambiente.

vi /etc/hosts
```
127.0.0.1 dev.giropops.local
127.0.0.1 staging.giropops.local
127.0.0.1 prod.giropops.local
```
![Title](imagens/ingress/dns.png)

Nos manifestos values, defini um parametro para declarar o endere√ßo DNS de cada Ingress.

values-dev.yaml
```
ingress:
  enabled: true
  host: dev.giropops.local # DNS
```
Agora o ingress ir√° apontar para o DNS **dev.giropops.local** que foi definido no arquivos **hosts** do Server.




--------------------















# HPA - Horizontal Pod Autoscaler.

√â um recurso nativo do kubernetes que ajusta automaticamente o n√∫mero de r√©plicas(pods) de um Deployment, ReplicaSet ou StatefulSet com base na utiliza√ß√£o
de recursos ou m√©tricas personalizadas.

Exemplo: Utiliza m√©tricas definidas em "resource" e "requests" dos containers para escalar.

Para o HPA funcionar, √© necess√°rio o Metrics Server instalado no Cluster.

### METRICS SERVER
r
**[METRICS SERVER](https://github.com/kubernetes-sigs/metrics-serve)** √© um agregador de m√©tricas de recursos de sistemas, que coleta m√©tricas como uso de CPU e mem√≥ria dos n√≥s e pods no Cluster.
Essas m√©tricas s√£o utilizadas no HPA para fazer o escalonamento dos Pods.



Instalando Metrics Server no Kind.
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl patch deployment metrics-server -n kube-system --type=json -p='[
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-insecure-tls"
  },
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/args/-",
    "value": "--kubelet-preferred-address-types=InternalIP"
  }
]'

```
![Title](imagens/HPA/install.png)



Verificando instala√ß√£o:
```
kubectl get pods -n kube-system | grep metrics-server
```
![Title](imagens/HPA/metrics.png)

Verificar se o Metrics Server est√° rodando corretamente:
```
kubectl get deployment metrics-server -n kube-system
kubectl logs -n kube-system deployment/metrics-server
```

Vamos verificar as m√©tricas de CPU dos Nodes:
````
kubectl top nodes
kubectl top pods -n dev
````
![Title](imagens/HPA/top.png)

Metrics Server Instalando e coletando dados.



**Deploy HPA:**

em /templetes criei um manifesto HPA.yaml que declara as especifica√ß√µes de AutoScaling.

Nos manifestos Values declarei um campo para definir as especifica√ß√µes do HPA. Esses valores s√£o carregados automaticamente pelo Helm quando rodo o Update.
```
hpa:
  enabled: true
  minReplicas: 1 #aqui defino o m√≠nimo de replicas.
  maxReplicas: 3 # aqui defino o m√°ximo de replicas.
  cpuUtilization: 80 #aqui defini o requisito m√≠nimo de CPU para ativar a regra.
  memoryUtilization: 95 #aqui defini o requisito m√≠nimo de mem√≥ria RAM para ativar a regra.
  targetDeployment: giropops-senhas # aqui estou apontando para meu deployment.
```
Primeiro busca o valor **hpa.targetDeployment** do values.yaml. Se n√£o estiver definido, cai no helper **giropops.fullname**.
```
scaleTargetRef:
  name: {{ .Values.hpa.targetDeployment | default (include "giropops.fullname" .) }}
```

Dentro do meu _helpers eu defini o nome da aplica√ß√£o em v√°riaveis:
```
{{- define "giropops.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{ .Values.fullnameOverride }}
{{- else -}}
{{ .Release.Name }}-{{ .Chart.Name }}
{{- end -}}
{{- end }}
```

Com esta maneira, posso chamar cada values de forma din√¢mica atrav√©s do:
```
{{ include "giropops.fullname" . }}
```

**HPA.yaml**
```
{{- if .Values.hpa.enabled }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ .Release.Name }}-hpa
  namespace: {{ .Release.Namespace }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ .Values.hpa.targetDeployment | default (include "giropops.fullname" .) }}
  minReplicas: {{ .Values.hpa.minReplicas }}
  maxReplicas: {{ .Values.hpa.maxReplicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ .Values.hpa.cpuUtilization }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{ .Values.hpa.memoryUtilization }}
{{- end }}
```

Vou atualizar o Helm nos ambientes Staging e Prod para aplicar o HPA.
```
helm upgrade --install giropops-staging . \
  --namespace staging \
  --values values-staging.yaml

helm upgrade --install giropops-prod . \
  --namespace prod \
  --values values-prod.yaml
```



Verificando M√©tricas de Prod e Staging:
```
kubectl get hpa -n staging
kubectl get hpa -n prod
```

![Title](imagens/HPA/hpatop.png)







--------------------------------------------



# LOCUST - TESTE DE CARGA 

[Locust](https://locust.io/) √© uma ferramenta open source escrita em Python para fazer testes de performance e carga.Voc√™ escreve scripts de teste em Python que simulam o comportamento de usu√°rios usando sua aplica√ß√£o.

Como o Locust Funciona?

    Voc√™ escreve um script Python descrevendo as a√ß√µes que cada "usu√°rio virtual" deve fazer (ex: logar, acessar p√°gina, fazer post, etc).

    Voc√™ executa o Locust ‚Äî que cria milhares de usu√°rios virtuais simulando essas a√ß√µes.

    Ele gera relat√≥rios interativos em tempo real via Web UI.

    Voc√™ analisa lat√™ncia, throughput, erros e muito mais.

Instala√ß√£o:

Em templates/ foi criado um manifesto locust-configmap.yaml com a fun√ß√£o de rodar um script(locustfile.py) para simular um teste de carga.

locust-configmap.yaml
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: locust-scripts
data:
  locustfile.py: |
    from locust import HttpUser, task, between

    class GiropopsLoadTest(HttpUser):
        wait_time = between(1, 3)  # Tempo de espera entre requisi√ß√µes

        @task
        def test_homepage(self):
            self.client.get("/")  # Simula requisi√ß√µes para a aplica√ß√£o

        @task
        def test_generate_password(self):
            self.client.get("/generate")  # Simula gera√ß√£o de senhas
```

locustfile.py
```
from locust import HttpUser, task, between

class Giropops(HttpUser):
    wait_time = between(1, 2)

    @task(1)
    def gerar_senha(self):
        self.client.post("/api/gerar-senha", json={"tamanho": 8, "incluir_numeros": True, "incluir_caracteres_especiais": True})


    @task(2)
    def listar_senha(self):
        self.client.get("/api/senhas")
```

Para Deployment do locust, foi criado um manifesto *locust-deployment.yaml* e um Service *locust-service.yaml*.

**locust-deployment.yaml**
```
{{- if .Values.locust.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: locust-giropops
  labels:
    app: locust-giropops
spec:
  replicas: {{ .Values.locust.replicas }}
  selector:
    matchLabels:
      app: locust-giropops
  template:
    metadata:
      labels:
        app: locust-giropops
    spec:
      containers:
      - name: locust-giropops
        image: {{ .Values.locust.image }}
        env:
          - name: LOCUST_LOCUSTFILE
            value: "/usr/src/app/scripts/locustfile.py"
        ports:
        - containerPort: 8089
        imagePullPolicy: Always
        volumeMounts:
        - name: locust-scripts
          mountPath: /usr/src/app/scripts
      volumes:
      - name: locust-scripts
        configMap:
          name: {{ .Values.locust.scriptConfigMap }}
          optional: true
{{- end }}
```

**locust-service.yaml**
```
{{- if .Values.locust.enabled }}
apiVersion: v1
kind: Service
metadata:
  name: locust-service
spec:
  selector:
    app: locust-giropops
  ports:
    - protocol: TCP
      port: 8089
      targetPort: 8089
  type: {{ .Values.locust.service.type }}
{{- end }}
```

Para ativar o Locust nos ambientes, declarei um campo para o Locust marcando como **enable: true** no ambiente de Staging, os demais ambientes deixei o par√¢metro como **false**.

**values-staging.yaml**
```
locust:
  enabled: true  # Habilita ou desabilita o Locust
  image: "linuxtips/locust-giropops:1.0"
  replicas: 1
  service:
    type: NodePort  # Pode ser LoadBalancer na nuvem
    port: 8089
  scriptConfigMap: "locust-scripts"  # Nome do ConfigMap com o locustfile.py
```

### TESTE DE CARGA - LOCUST - HPA

Agora irei verificar o HPA junto com o LOCUST realizando um teste de carga.

No manifesto values-staging.yaml, defini o valor m√°ximo de 10 Pods.
```
hpa:
  enabled: true 
  minReplicas: 1
  maxReplicas: 10
```

Primeiro irei verificar o nome do Pod do Locust e expor via **Port-Foward**.
```
kubectl get pods -n staging -l app=locust-giropops
```
```
kubectl port-forward -n staging pod/locust-giropops-75b9ff794d-dnwbq 8089:8089
```
![Title](imagens/locust/portfowardlocust.png)


Locust acessado, irei simular um teste de carga.

**Teste: a cada segundo, 20 novos usu√°rios come√ßam a usar a aplica√ß√£o at√© chegar em 1000 usu√°rios.**

![Title](imagens/locust/locust.png)

![Title](imagens/locust/locustexec.png)


Aqui podemos ver o HPA entrando em a√ß√£o ap√≥s o estresse do Locust, Os Pods existentes bateram a tigger de limite de CPU e Mem√≥ria e come√ßaram
a criar novos Pods.

![Title](imagens/locust/podescalando.png)

![Title](imagens/locust/dados.png)





-------------------------




# COSIGN - IMAGENS ASSINADAS E SEGURAS

[Cosign](https://github.com/sigstore/cosign)  √© uma ferramenta de linha de comando desenvolvida como parte do projeto Sigstore. Ela √© usada para assinar, verificar, armazenar e recuperar artefatos de software, particularmente imagens de container, atrav√©s de interfaces com registradores OCI (Open Container Initiative)


Instala√ß√£o Cosign:
```
COSIGN_VERSION=$(curl -s https://api.github.com/repos/sigstore/cosign/releases/latest | grep tag_name | cut -d '"' -f 4)

curl -Lo cosign https://github.com/sigstore/cosign/releases/download/${COSIGN_VERSION}/cosign-linux-amd64

chmod +x cosign

sudo mv cosign /usr/local/bin/
```

```
cosign version
```
![Title](imagens/cosign/version.png)



Cosign instalado, agora irei come√ßar o processo de assinatura de imagem.

Vou gerar um par de chaves:
```
cosign generate-key-pair
```
![Title](imagens/cosign/key.png)

Agora possuo 2 chaves, uma privada e outra p√∫blica.

![Title](imagens/cosign/keys.png)

Assinando imagem:
```
cosign sign --key cosign.key docker.io/geforce8400gsd/giropops-senhas:latest
```

Imagem assinada.
![Title](imagens/cosign/assinado.png)


Verificando assinatura:
```
cosign verify --key cosign.pub docker.io/geforce8400gsd/giropops-senhas:latest
```

### Secret - Cosign


```
kubectl create secret generic cosign-pub \
  --from-file=cosign.pub=/home/pick/PICK/.github/cosign/cosign.pub \
  -n cert-manager
```
![Title](imagens/cosign/secret.png)


# KUBE PROMETHEUS 

[Kube Prometheu](https://github.com/prometheus-operator/kube-prometheus) √© uma cole√ß√£o de componentes para instalar e configurar um stack completo de monitoramento no Kubernetes, feito pela comunidade Prometheus + CoreOS.


```
Componente	Fun√ß√£o
Prometheus	Coleta e armazena m√©tricas (CPU, mem√≥ria, requests, etc.)
Grafana	Interface para dashboards lind√µes üé®
Alertmanager	Envia alertas (Slack, email, etc)
Node Exporter	Exporta m√©tricas do n√≥ (CPU/disk/etc.)
kube-state-metrics	M√©tricas do estado dos recursos K8s
Prometheus Operator	Facilita deploys de Prometheus via CRDs
```


Iniciando a instala√ß√£o Kube Prometheus, irei adicionar as CDR (Custom Resource Definition) no HELM:
````
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
````
Criando Namespace:
````
kubectl create namespace monitoring
````
Agora irei iniciar a instala√ß√£o do *kube prometheus stack* na namespace *monitoring*.
```
helm install kube-prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set grafana.enabled=true \
  --set prometheus.resources.limits.cpu=100m \
  --set prometheus.resources.limits.memory=128Mi \
  --set grafana.resources.limits.cpu=100m \
  --set grafana.resources.limits.memory=128Mi
```

Verificando a instala√ß√£o:
````
kubectl get pods -n monitoring
kubectl get svc -n monitoring
````

![Title](imagens/monitoring/install.png)


![Title](imagens/monitoring/svcpod.png)

Pods e Services rodando com sucesso, agora irei criar o **Service Monitor**.

### Service Monitor

[Service monitor](https://observability.thomasriley.co.uk/prometheus/configuring-prometheus/using-service-monitors/) √© uma Custom Resource Definition (CRD) usado pelo Prometheus Operator no kubernetes...

Ele j√° vem instalado no kube-prometheus. O Kube-prometheus j√° vem com v√°rios ServiceMonitors configurados.
Para visualizar os servicemonitors:
````
kubectl get servicemonitors -n monitoring
````

Criei um manifesto para o service monitor para monitorar o ambiente de Produ√ß√£o.

**servicemonitor-prod.yaml**
````
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: giropops-senhas-servicemonitor
  namespace: monitoring  # mesmo namespace do Prometheus
  labels:
    release: kube-prometheus  # importante se Prometheus usa labelSelector
spec:
  selector:
    matchLabels:
      app: giropops-senhas  # deve bater com o .Values.services.giropops-senhas.labels.app
  namespaceSelector:
    matchNames:
      - dev  # ou "prod", dependendo de onde est√° seu app
  endpoints:
    - port: giropops-senhas-metrics  # mesmo nome usado no service
      path: /metrics                 # endpoint exposto no app
      interval: 15s
````

Aplicando o manifest:
```
kubectl apply -f servicemonitor-prod.yaml
```


### Nginx Ingress e DNS - EXPONDO GRAFANA E PROMETHEUS

Para expor o Grafana e Prometheus, criei 2 manifestos ingress que apontam para os services *kube-prometheus-grafana* e *kube-prometheus-kube-prome-prometheus*.

**ingress-grafana.yaml**
````
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: grafana.giropops.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-grafana
                port:
                  number: 80
````

**ingress-prometheus.yaml**
````
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: prometheus.giropops.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kube-prometheus-prometheus
                port:
                  number: 9090

````

Para o DNS, adicionei **prometheus.giropops.local** e **grafana.giropops.local** no arquivo **/etc/hosts**.

![Title](imagens/monitoring/dnsprome.png)


Aplicando Ingress:
````
kubectl apply -f ingress-prometheus.yaml
kubectl apply -f grafana.yaml
````

Prometheus e Grafana acessados com sucesso.

![Title](imagens/monitoring/prografana.png)







# Cert-Manager 

O [Cert-Manager](https://github.com/Marcusronney/cert-manager) √© um controlador de certificados para Kubernetes que automatiza a emiss√£o, renova√ß√£o e gerenciamento de certificados TLS ‚Äî de forma segura e integrada ao cluster.


Instalando:
```
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
```

Verificando os PODs:
```
kubectl get all --namespace cert-manager
```
![Title](imagens/cert-manager/cert.png)


Em ambientes locais como o Kind, irei usar o **SelfSigned** para emitir os certificado pois meu cluster local n√£o DNS p√∫blico nem acesso externo pela WAN. Certificados Self-Singed s√£o certificados auto assinados pelo Cluster, basicamente o Cluster vira uma CA.


Nesta etapa, estou definindo um manifesto para deploy do Issue selfSigned + Certificado CA.

Emitindo a CA e criando um certificado autofirmado (self-signed):

**selfsigned-giropops-prod.yaml**
````
apiVersion: cert-manager.io/v1
kind: Issuer #um emissor de certificados
metadata:
  name: selfsigned-issuer
  namespace: prod
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: giropops-ca
  namespace: prod
spec:
  isCA: true
  secretName: giropops-ca-secret
  commonName: giropops.local
  issuerRef:
    name: selfsigned-issuer
    kind: Issuer
````


Agora irei criar o manifesto Issue do tipo CA.
Esse Issuer permite que o cert-manager emita certificados TLS usando uma CA (Autoridade Certificadora) 
existente, que est√° armazenada no cluster dentro de um Secret TLS.

**inssuer-ca.yaml**
```
apiVersion: cert-manager.io/v1 #Vers√£o da API do cert-manager
kind: Issuer #um emissor de certificados
metadata:
  name: giropops-ca-issuer #Nome do emissor
  namespace: prod #namespace
spec:
  ca:
    secretName: giropops-ca-secret #Nome do Secret TLS que possu√≠ a CA e a key.
```


Por √∫ltimo, irei definir o manifesto para o certificate apontando para prod.giropops.local.

**certificado-giropops.yaml**
````
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: giropops-cert
  namespace: prod
spec:
  secretName: giropops-tls
  dnsNames:
    - giropops.local
  issuerRef:
    name: giropops-ca-issuer
    kind: Issuer
`````

Aplicando:
```
kubectl apply -f selfsigned-giropops-prod.yaml
kubectl apply -f inssuer-ca.yaml
kubectl apply -f certificado-giropops.yaml
```

![Title](imagens/cert-manager/apply.png)

Fluxo:
````
[ giropops-ca-issuer (Issuer) ]
        ‚Üì
[ giropops-cert (Certificate) ]
        ‚Üì
[ giropops-tls (Secret TLS com cert + key) ]
        ‚Üì
[ Ingress TLS ‚Üí HTTPS habilitado em giropops.local ]
````

Vou verificar as CA no namespace prod:
```
kubectl get issuer -n prod
kubectl get certificate -n prod
kubectl get secret giropops-tls -n prod
```
![Title](imagens/cert-manager/ca.png)

Agora j√° consigo visualizar o Secret giropops-tls do meu cluster.

Extraindo ca.crt e .crt:




giropops.ca.crt
````
kubectl get secret giropops-ca-secret -n prod -o jsonpath="{.data['ca\.crt']}" | base64 -d > giropops-ca.crt
````



giropops.crt
````
kubectl get secret giropops-tls -n prod -o jsonpath="{.data['tls\.crt']}" | base64 -d > giropops.crt
````
Podemos verificar os certicados giropops-ca.crt e giropops.crt
````
#ls
certificado-giropops.yaml  giropops-ca.crt  giropops.crt  inssuer-ca.yaml  selfsigned-giropops-prod.yaml`
````
**giropops.crt** = Chave privada

**giropops.ca.crt** = Chave p√∫blica




Verificando o CN:
````
openssl x509 -in giropops-ca.crt -noout -text -issuer -subject -dates
````
![Title](imagens/cert-manager/cn.png)


No manifesto ingress.yaml, foi passado v√°riaveis para percorrer o values-prod.yaml
````
  tls:
  - hosts:
      - {{ .Values.ingress.host }}
    secretName: {{ .Values.ingress.tlsSecretName }}
````

No values-prod.yaml, defini os valores do SecretName, para se conectar ao CA criado.
````
  tlsSecretName: giropops-tls
  issuerName: giropops-ca-issuer
````

Resumindo, o Cert-Maneger atraves dos manifests emitiu o Secret com cert e chave. O Ingress aponta para este Secret e o Nginx Controlller serve o certificado para o Host.

Realizando teste do certificado:

`````
curl -v https://prod.giropops.local --cacert giropops-ca.crt
`````
![Title](imagens/cert-manager/curlcert.png)

Certificado V√°lidado com sucesso.

**Detalhe**: Como meu Cluster √© local,as outras m√°quinas na rede n√£o conhecem e nem confiam na minha CA. Neste caso, irei instalar manualmente o certificado **giropops-ca.crt** no host.

Uma maneira simples de coletar o certificado dentro do cluster √© usando o SCP no PowerShell do Windows.

````
scp root@192.168.1.81:/home/pick/PICK/.github/workflows/cert-manager/giropops-ca/ca.crt $HOME\Downloads\giropops-ca.crt
````
![Title](imagens/cert-manager/windows.png)


Certificado instalado e v√°lido.
![Title](imagens/cert-manager/certificadovalido.png)











# KYVERNO - Pol√≠ticas de Seguran√ßa


[Kyverno](https://kyverno.io/docs/) √© uma ferramenta de gerenciamento de pol√≠ticas para Kubernetes, ele trabalha como um Policy Engine servindo para aplicar regras de seguran√ßa, validar configura√ß√µes e automatizar corre√ß√µes dentro de clusters.

Funcionalidades:
Valida√ß√£o e Muta√ß√£o de Recursos.
Gerenciamento de Pol√≠ticas.
Relat√≥rios e Exce√ß√µes.
Verifica√ß√£o de Assinaturas de Imagens.


Instala√ß√£o:

Adicionando Repo ao HELM:
```
helm repo add kyverno https://kyverno.github.io/kyverno/
helm repo update
```

Criando Namespace **kyverno** e instalando os pacotes do kyverno:
````
kubectl create namespace kyverno

helm install kyverno kyverno/kyverno --namespace kyverno
````
![Title](imagens/kyverno/kyverno.png)

Verificando CRD.
```
kubectl get crd | grep kyverno
```
![Title](imagens/kyverno/crdkyverno.png)

Verificando Pods da namespace kyverno:
````
kubectl get pods -n kyverno
````
![Title](imagens/kyverno/pod.png)



Kyverno Instalado com sucesso!



Criando as Pol√≠ticas de Seguran√ßa:

### verificando-assinaturas-images-cosign.yaml

Esta Policy s√≥ permite criar Pods com imagens assinadas pela chave p√∫blica Cosign

```
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verificando-assinaturas-images
spec:
  validationFailureAction: Enforce
  background: true
  rules:
    - name: verify-signed-images
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - prod
      verifyImages:
        - imageReferences:
            - "docker.io/geforce8400gsd/*"
          key: "k8s://cert-manager/cosign-pub"
          attestations: [] #Neste campo, estamos definindo que a chave est√° no cluster, o namespace onde procurar e o nome da chave.
```

### desabilitando-root.yaml

````
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: desabilitando-root
spec:
  validationFailureAction: Enforce
  background: true
  rules:
    - name: no-run-as-root
      match:
        any:
          - resources:
              kinds:
                - Pod
              namespaces:
                - prod
      validate:
        message: "Rodar como root √© proibido na namespace prod."
        pattern:
          spec:
            securityContext:
              runAsNonRoot: true
            containers:
              - securityContext:
                  runAsNonRoot: true
````

### bloqueando-env.yaml

Bloqueia a cria√ß√£o de Pods na namespace prod se eles tiverem vari√°veis de ambiente sens√≠veis declaradas nos containers, como:

````
PASSWORD

SECRET

TOKEN

AWS_ACCESS_KEY

AWS_SECRET_KEY
````

````
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: block-sensitive-env-vars-prod
spec:
  validationFailureAction: enforce # Quando violado, o bloqueio √© realizado
  background: true #aplica a pods existentes
  rules:
    - name: bloqueando-env
      match:
        any: #aqui, estou setando para a namespace Prod
          - resources:
              kinds:
                - Pod
              namespaces:
                - prod
      validate:
        message: "Vari√°veis de ambiente sens√≠veis n√£o s√£o permitidas em 'prod'."
        deny:
          conditions:
            any:
              - key: "{{ request.object.spec.containers[].env[].name }}"
                operator: AnyIn
                value:#bloqueio
                  - PASSWORD
                  - SECRET
                  - TOKEN
                  - AWS_ACCESS_KEY
                  - AWS_SECRET_KEY
````

### compliance.yaml

Essa policy for√ßa que Deployments da namespace prod:

    N√£o usem root

    N√£o pe√ßam privil√©gios extras

    N√£o modifiquem seu filesystem

    N√£o carreguem capabilities perigosas

````
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: compliance
spec:
  validationFailureAction: enforce # faz um bloqueio se n√£o for conforme a regra
  background: true #Tamb√©m verifica recursos j√° existentes, n√£o s√≥ novos
  rules:
    - name: compliance-padr√µes-m√≠nimos
      match:
        any:
          - resources:
              kinds:
                - Deployment
              namespaces:
                - prod  #atacando todos os Pods da namespace prod
      validate:
        message: "Deployments devem seguir padr√µes de seguran√ßa m√≠nimos."
        pattern:
          spec:
            template:
              spec:
                securityContext:
                  runAsNonRoot: true #n√£o-root
                containers:
                  - securityContext:
                      runAsNonRoot: true
                      allowPrivilegeEscalation: false #sem escala√ß√£o de privil√©gios
                      capabilities:
                        drop:
                          - ALL #aqui, defini para bloquear todas as capacidades de ADMIN
                      readOnlyRootFilesystem: true # O Filesystem foi definido como somente leitura
````

### resources-limits.yaml



````
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: resources-limits
spec:
  validationFailureAction: enforce #Neste argumento, um bloqueio √© realizado em caso de viola√ß√£o.
  rules:
  - name: validando-limites-prod
    match:
      any:
        - resources:
            kinds:
              - Pod #atacando todos os Pods
            namespaces:
              - prod #namespace prod
    validate:
      message: "Precisa definir o limite de recursos de CPU e Mem√≥ria RAM para todos os Pods de Produl√ßao"
      pattern:
        spec:
          containers:
          - name: "*" #Wildcard, assim, a policy pega em todos os Containers.
            resources:
              limits:
                cpu: "?*" # Obrigat√≥rio uso de argumento para CPU
                memory: "?*" #Obrigat√≥rio uso de argumento para Mem√≥ri
````

Aplicando as Policys:
````
kubectl apply -f verificando-assinaturas-images-cosign.yaml
kubectl apply -f desabilitando-root.yaml
kubectl apply -f bloqueando-env.yaml
kubectl apply -f compliance.yaml
kubectl apply -f resources-limits.yaml
````





![Title](imagens/kyverno/policy.png)



### Realizando testes de Policys.

No diret√≥rio /kyverno/testes-Policys, criei alguns manifestos para testar as policys aplicadas.

Note que o deploy foi bloqueado por conta das regras **compliance**, **desabilitando-root** e **resources-limits**.
![Title](imagens/kyverno/test-assinatura.png)

![Title](imagens/kyverno/test-root.png)

Outros exemplos do Kyverno entrando em a√ß√£o:

![Title](imagens/kyverno/testcompliance.png)

--------------------










# CI/CD - GitHub Actions

![github](https://img.icons8.com/?size=100&id=3tC9EQumUAuq&format=png&color=000000)





















